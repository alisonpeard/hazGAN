# training settings -- defaults from Gulrajani et al. 2017 (CFAR-10 model)
epochs:
  desc: Number of epochs to train for
  value: 1000
train_size:
  value: 0.8 # 840 # 50
batch_size:
  value: 64 # for GPU constraints
fields:
  desc: Which fields to model, subsets of ['u10', 'tp', 'mslp']
  value: ['u10', 'mslp']
label_ratios:
  desc: Dataset representation, by value
  value:
    'pre': 0.3333333
    '15':  0.3333333
    '999': 0.3333333
augment_policy:
  desc: Augmentation policies (https://arxiv.org/pdf/2006.10738),  ['color,translation,cutout', '']
  value: 'color,translation,cutout'
condition:
  desc: Whether to use conditioning, bool
  value: True
labels:
  desc: Whether to use labels, bool
  value: True
embedding_depth:
  desc: How many channels to project the label to
  value: 32
seed:
  value: 42

# training features
training_balance:
  desc: Gulrajani et al. 2017 use 5
  value: 5
latent_space_distn:
  desc: Distribution of latent space, ['normal', 'uniform', 'gumbel']
  value: gumbel
gumbel:
  desc: Train on Gumbel marginals instead of uniform marginals
  value: True
lambda_gp:
  value: 10.

# architecture
lrelu:
  desc: Gulrajani et al. 2017 use 0.2
  value: 0.2
dropout:
  desc: Gulrajani et al. 2017 don't use dropout, neither does Shruti
  value: 0.
latent_dims:
  desc: Gulrajani et al. 2017 use 128
  value: 128
generator_width:
  desc: Maximum filters in any layer of the generator. 
  value: 256
critic_width:
  desc: Maximum filters in any layer of the critic.
  value: 256
interpolation:
  desc: Interpolation method for upsampling, ['nearest', 'bilinear'] (https://distill.pub/2016/deconv-checkerboard/)
  value: bilinear
normalize_generator:
  desc: Whether to batch normalise generator blocks.
  value: True
normalize_critic:
  desc: Whether to layer normalise critic blocks. NB this matters for conditioning.
  value: True

# optimizer
optimizer:
  desc: Optimizer to use, ['SGD', 'Adam']
  value: Adam
learning_rate:
  desc: Gulrajani et al. 2017 use 0.0001 (Appendix B)
  value: 0.001
beta_1:
  desc: Gulrajani et al. 2017 use 0.5, [0, 0.5]
  value: 0.5
beta_2:
  desc: Gulrajani et al. 2017 use 0.9, [0.9, 0.99]
  value: 0.9
weight_decay:
  desc: Gulrajani et al. 2017 use 10-3
  value: 0.001
use_ema:
  desc: Use exponential moving average in training, causes issues when re-loading weights.
  value: True # only set to true if not loading weights
ema_momentum:
  desc: Momentum of exponential moving average for tf.keras.optimizers.Adam (higher --> smoother)
  value: 0.999
ema_overwrite_frequency:
  desc: How often [batches] to overwrite weights with ema
  value: 2
