# training settings -- defaults from Gulrajani et al. 2017 (CFAR-10 model)
epochs:
  desc: Number of epochs to train for
  value: 100
train_size:
  value: 0.8 # 840 # 50
batch_size:
  value: 64 # for GPU constraints
fields:
  desc: Which fields to model, subsets of ['u10', 'tp', 'mslp']
  value: ['u10', 'mslp']
label_ratios:
  desc: Dataset representation, by value
  value:
    pre: 0.1
    '7': 0.4
    '20': 0.5
augment_policy:
  desc: Augmentation policies (https://arxiv.org/pdf/2006.10738),  ['color,translation,cutout', '']
  value: 'color,translation,cutout'
condition:
  desc: Whether to use conditioning, bool
  value: True
labels:
  desc: Whether to use labels, bool
  value: True
embedding_depth:
  desc: How many channels to project the label to
  value: 32
seed:
  value: 42

# training features
optimizer:
  desc: Optimizer to use, ['SGD', 'Adam']
  value: Adam
training_balance:
  desc: Gulrajani et al. 2017 use 5
  value: 5
latent_space_distn:
  desc: Distribution of latent space, ['normal', 'uniform', 'gumbel']
  value: gumbel
gumbel:
  desc: Train on Gumbel marginals instead of uniform marginals
  value: True
uniform:
  desc: Train on uniform marginals from ECDF or semi-parametric model
  value: uniform
lambda_gp:
  value: 10.
lambda_condition:
  desc: Weight of penalty on max wind condition
  value: 0.01

# architecture
lrelu:
  desc: Gulrajani et al. 2017 use 0.2
  value: 0.2
dropout:
  desc: Gulrajani et al. 2017 don't use dropout, neither does Shruti
  value: 0.
latent_dims:
  desc: Gulrajani et al. 2017 use 128
  value: 128
g_layers:
  desc: Number of channels in the hidden layers for the generator. [256, 128, 64]
  value: [256, 128, 64]
d_layers:
  desc: Number of channels in the hidden layers for the discriminator. [128, 256, 512]
  value: [64, 128, 256]
interpolation:
  desc: Interpolation method for upsampling, ['nearest', 'bilinear'] (https://distill.pub/2016/deconv-checkerboard/)
  value: bilinear
normalize_generator:
  desc: Whether to batch normalise generator blocks.
  value: True
normalize_critic:
  desc: Whether to layer normalise critic blocks. NB this matters for conditioning.
  value: True

# Adam parameters https://doi.org/10.48550/arXiv.1704.00028
learning_rate:
  desc: Gulrajani et al. 2017 use 0.0001 (Appendix B)
  value: 0.001
beta_1:
  desc: Gulrajani et al. 2017 use 0.5, [0, 0.5]
  value: 0.5
beta_2:
  desc: Gulrajani et al. 2017 use 0.9, [0.9, 0.99]
  value: 0.9
weight_decay:
  desc: Gulrajani et al. 2017 use 10-3
  value: 0.001
use_ema:
  desc: Use exponential moving average in training, causes issues when re-loading weights.
  value: True # only set to true if not loading weights
ema_momentum:
  desc: Momentum of exponential moving average for tf.keras.optimizers.Adam (higher --> smoother)
  value: 0.999
ema_overwrite_frequency:
  desc: How often [batches] to overwrite weights with ema
  value: 2
