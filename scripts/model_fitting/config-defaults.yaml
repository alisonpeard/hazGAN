# training settings
model:
  desc: Which model to train, ['wgan' | 'dcgan'].
  value: wgan
nepochs:
  value: 500 #30000 or 1,500,000/train_size
train_size:
  value: 560 # 50
batch_size:
  value: 32
chi_frequency:
  desc: How often to calculate chi-score for train and test.
  value: 10

# training features
lr_factor:
  value: 0.5
lr_patience:
  value: 20
lambda_gp:
  value: 10.
lambda_chi:
  value: 0
training_balance:
  desc: How many more times to train discriminator than generator.
  value: 5
true_label_smooth:
  desc: Multiply true labels by this to smooth discriminator's labels.
  value: 0.9
latent_space_distn:
  desc: Distribution of latent space, ['normal', 'uniform', 'gumbel', 't'].
  value: gumbel
gumbel:
  desc: Train on Gumbel marginals instead of uniform marginals.
  value: True
optimizer:
  desc: Optimizer to use, ['SGD', 'Adam'].
  value: Adam

# architecture
lrelu:
  value: 0.2
dropout:
  value: 0.4
latent_dims:
  value: 128
g_layers:
  desc: Number of channels in the hidden layers for the generator.
  value: [25600, 512, 256]
d_layers:
  desc: Number of channels in the hidden layers for the discriminator.
  value: [128, 256, 512]
g_complexity:
  value: 1
d_complexity:
  value: 1
interpolation:
  desc: Interpolation method for upsampling, ['nearest', 'bilinear'] (https://distill.pub/2016/deconv-checkerboard/)
  value: nearest

# Adam parameters https://doi.org/10.48550/arXiv.1704.00028
learning_rate:
  value: 0.0001
beta_1:
  value: 0 
beta_2:
  value: 0.9 
weight_decay:
  value: 0.01
global_clipnorm:
  value: False
use_ema:
  desc: Use exponential moving average in training, causes issues when re-loading weights.
  value: True # only set to true if not loading weights
ema_momentum:
  value: 0.9
ema_overwrite_frequency:
  desc: How often to overwrite weights with ema.
  value: 1

# for linux cluster
# export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soge-home/users/spet5107/micromamba/envs/tensorflow/lib